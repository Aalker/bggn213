---
title: "Class 9 Unsupervised learning"
author: "Amanda Alker"
date: "5/2/2018"
output: 
  html_document: 
    keep_md: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Read the csv file from the webpage named "url"
```{r}
url <- "https://bioboot.github.io/bggn213_S18/class-material/WisconsinCancer.csv"

wisc.df <- read.csv(url)
head(wisc.df)
```

How may diagnonsis are cancer vs non cancer?
```{r}
table(wisc.df$diagnosis)
```

Lets make a new data matrix with just the numeric values of interest in it. So we want to get rid of the first 3 columns.
```{r}
wisc.data <- as.matrix(wisc.df[ ,3:32])

rownames(wisc.data) <- wisc.df$id

#Inverse option- select all but the ones found
#x <- c("barry", "chris", "mary", "pete")
#x[-c(1,4)]
```


Q1. How many observations are in this dataset?
569 (number of samples) nrow(wisc.df)

```{r}
#number of rows
nrow(wisc.data)

#dimensions
dim(wisc.data)

```

Q2. How many variables/features in the data are suffixed with _mean?
10

```{r}
#number of columns
ncol(wisc.data)

#pulls strings based on pattern matching, pattern, x
grep("_mean", colnames(wisc.data), value = TRUE)

length(grep("_mean", colnames(wisc.data), value = TRUE))
#invert = TRUE (returns what you dont want)
```


Q3. How many of the observations have a malignant diagnosis?

```{r}
#create diagnosis vector 
#use equality check on column 

diagnosis <- as.numeric(wisc.df$diagnosis == "M")
sum(diagnosis)
```

```{r}
colMeans(wisc.data)

# lets plot the data to see if we need to do scaling
plot(colMeans(wisc.data), type = "o")

#take any function and apply over columns and run to see if SD the same? Do we need to do scaling?
apply(wisc.data, 2, sd)
```

Make the principal Component Analysis
```{r}

wisc.pr <- prcomp(wisc.data, scale = TRUE)

#how are we doing?
summary(wisc.pr)
```

### PCA results plot

```{r}
#look at the different irgone value plots to start to get an idea of how the data will seperate 
plot(wisc.pr$x[,1], wisc.pr$x[,2], col= diagnosis+1)
```

Now let's look at the loadings to determine which PCs are important

## Scree-plot (elbow plot): variance explained
```{r}
#See the 'Variance explained' part in the handout
pr.var <- wisc.pr$sdev^2
pve <- pr.var/ sum(pr.var)
```

```{r}
# Plot variance explained for each principal component
plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")
```


```{r}
# Alternative scree plot of the same data, note data driven y-axis
#data driven axis tries to fit the axis to your data-- fix this by creating a custom axis
#at=ave puts the ticks where your datapoints lie

barplot(pve, ylab = "Precent of Variance Explained",
     names.arg=paste0("PC",1:length(pve)), las=2, axes = FALSE)
axis(2, at=pve, labels=round(pve,2)*100 )
```

Q9. For the first principal component, what is the component of the loading vector (i.e. wisc.pr$rotation[,1]) for the feature  concave.points_mean?


Q10. What is the minimum number of principal components required to explain 80% of the variance of the data?
Look at the summary(wisc.pr)- cumulative proportion- PC 3 

```{r}
# Plot cumulative proportion of variance explained
plot(cumsum(pve), xlab = "Principal Component", 
     ylab = "Cumulative Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")
```


###Heirarchical Clustering 

```{r}
#Scale the data
data.scaled <- scale(wisc.data)
```

```{r}
#calculate the Euclidean distance
data.dist <- dist(data.scaled)
#data.dist
```

Plot the hclust
```{r}
wisc.hclust <- hclust(data.dist, method = "complete")
plot(wisc.hclust)
abline(h = 20, col = "red", lwd = 3)
```

Selecting the number of clusters
```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, k = 4)
```

###How do these groups match our 'diagnosis'

```{r}
table(diagnosis)
```

```{r}
table(wisc.hclust.clusters)
```


Gives the intersection of these two data components and how they are represented in the cluster
```{r}
table( wisc.hclust.clusters, diagnosis)
```

### K-Means Clustering

Input scaled values here

```{r}
# centers = k = clusters
#nstart = iterations
data.scaled <- scale(wisc.data)
wisc.km <- kmeans(data.scaled , centers = 2, nstart = 20)
```

Cluster membership vector

```{r}
table(wisc.km$cluster)
```

Compare to expert 'diagnosis'

```{r}
table(wisc.km$cluster, diagnosis)
```

###Clustering on PCA results

```{r}
#clusterin results of principal components analysis
wisc.pr.hclust <- hclust(dist(wisc.pr$x[ , 1:3]), method = "ward.D2")
plot(wisc.pr.hclust)
```


```{r}
#cut into 4 clusters
wisc.pr.hclust.clusters <- cutree(wisc.pr.hclust, k=4)

plot(wisc.pr$x[,1:2], col = wisc.pr.hclust.clusters)
```

 ^^^ Produces better signal to noise ratio and allows to filter out the more important components  

```{r}
table(wisc.pr.hclust.clusters, diagnosis)
```

```{r}
#install.packages("rgl")
#library(rgl)
```

```{r}
#plot3d(wisc.pr$x[, 1:3], )
```

```{r}
url <- "https://tinyurl.com/new-samples-CSV"
new <- read.csv(url)
npc <- predict(wisc.pr, newdata = new)
```

New function 'predict'
```{r}
plot(wisc.pr$x[, 1:2], col=wisc.pr.hclust.clusters)
points(npc[, 1], npc [, 2], col = c("orange", "blue"), pch = 16, cex = 3)
```

